# 1ï¸âƒ£ **TOKEN-BUCKET ALGORITHM** ğŸª£


```go 
package main

import (
	"fmt"
	"time"
)

// "Token Bucket Algorithm" implementation âš¡
// Why struct{}{} ? - No memory-overhead with an empty struct{} (0 bytes) ğŸ’¡

type RateLimiter struct {
	tokens     chan struct{}
	refillTime time.Duration
}

 func NewRateLimiter(rateLimit int, refillTime time.Duration) *RateLimiter{
	rl:= &RateLimiter{
	tokens: make(chan struct{}, rateLimit),
	refillTime: refillTime,
}
	for range rateLimit{
	rl.tokens <- struct{}{}
}
	go rl.startRefill() // goroutine running in the BG.
	return  rl
 }

 func (rl *RateLimiter) startRefill(){
	ticker:= time.NewTicker(rl.refillTime)
	defer ticker.Stop()
	for {
		select {
		case <-ticker.C:
			select {
			case rl.tokens <-struct{}{}:
			default:
			}
		}
	}
} 

func (rl *RateLimiter) allow() bool{
	select {
	case <-rl.tokens:
		return true
	default:
		return false	
	}
}

func main() {
	rateLimiter:= NewRateLimiter(5,time.Second) // 5 requests

	// Let's send 10 requests
	for range 10{
		if rateLimiter.allow(){
			fmt.Println("Request Allowed âœ…")
		}else{
			fmt.Println("Request denied âŒ")
		}
		time.Sleep(200 * time.Millisecond) // some delay
	}

}

// OUTPUT: (200 ms. interval.. â±ï¸)
// $ go run .
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request denied âŒ
// Request denied âŒ
// Request denied âŒ
// Request denied âŒ
```

---
## ğŸ§  **Concept Recap: Token Bucket Algorithm**

* Itâ€™s a **rate-limiting algorithm** that controls how many operations (e.g., API requests) can happen per unit time.
* Think of it like a **bucket** that holds â€œtokens.â€
* Each token = 1 allowed request.
* When a request arrives:

  * If thereâ€™s a token â†’ take one out â†’ request allowed âœ…
  * If bucket empty â†’ request denied âŒ
* Tokens refill over time at a steady rate.

---

## âš™ï¸ **Our Code Walkthrough**

### 1ï¸âƒ£ Struct Definition

```go
type RateLimiter struct {
	tokens     chan struct{}
	refillTime time.Duration
}
```

* `tokens`: A **channel** of type `struct{}` used as the token bucket.

  * Why `struct{}{}`?

    * It takes **0 bytes** â€” memory-efficient.
    * We only care about the *count*, not data.
* `refillTime`: How often tokens refill.

---

### 2ï¸âƒ£ Constructor Function

```go
func NewRateLimiter(rateLimit int, refillTime time.Duration) *RateLimiter {
	rl := &RateLimiter{
		tokens: make(chan struct{}, rateLimit),
		refillTime: refillTime,
	}
	for range rateLimit {
		rl.tokens <- struct{}{}
	}
	go rl.startRefill()
	return rl
}
```

**Step-by-step:**

1. Creates a channel (`rl.tokens`) with buffer size = `rateLimit` (i.e., max tokens).
2. Fills the channel to capacity using:

   ```go
   for range rateLimit {
       rl.tokens <- struct{}{}
   }
   ```

   â†’ This simulates a **bucket initially full** of tokens.
3. Launches a **goroutine** `startRefill()` â†’ constantly refills tokens periodically.
4. Returns the new `RateLimiter` instance.

---

### 3ï¸âƒ£ Background Token Refill

```go
func (rl *RateLimiter) startRefill() {
	ticker := time.NewTicker(rl.refillTime)
	defer ticker.Stop()
	for {
		select {
		case <-ticker.C:
			select {
			case rl.tokens <- struct{}{}:
			default:
			}
		}
	}
}
```

**Detailed breakdown:**

* `time.NewTicker(rl.refillTime)` creates a **ticker** that ticks every `refillTime`.
* Each tick triggers an attempt to **add a token** back into the bucket:

  ```go
  select {
  case rl.tokens <- struct{}{}:
  default:
  }
  ```

  * Inner `select` tries to send a token.
  * If the channel is already full (bucket full) â†’ `default` executes â†’ skip (no blocking).

âœ… **Key Point:**
This ensures the bucket *never overflows* â€” it stays capped at its maximum (`rateLimit`).

---

### 4ï¸âƒ£ Request Allow Function

```go
func (rl *RateLimiter) allow() bool {
	select {
	case <-rl.tokens:
		return true
	default:
		return false	
	}
}
```

* **Try to take a token** from the channel:

  * If available â†’ remove one â†’ request allowed âœ…
  * If empty â†’ skip `default` â†’ request denied âŒ
* The `select` with `default` makes it **non-blocking** â€” it immediately decides.

---

### 5ï¸âƒ£ Main Function

```go
func main() {
	rateLimiter := NewRateLimiter(5, time.Second) // 5 requests

	for range 10 {
		if rateLimiter.allow() {
			fmt.Println("Request Allowed âœ…")
		} else {
			fmt.Println("Request denied âŒ")
		}
		time.Sleep(200 * time.Millisecond)
	}
}
```

**Logic:**

* Bucket capacity = 5 tokens.
* Refill rate = 1 token per second.
* Loop sends 10 requests, every 200ms.

---

## ğŸ§© **Execution Timeline**

| Time (approx) | Request # | Tokens before   | Allowed? | Tokens after | Explanation          |
| ------------- | --------- | --------------- | -------- | ------------ | -------------------- |
| 0s            | 1         | 5               | âœ…        | 4            | Tokens available     |
| 0.2s          | 2         | 4               | âœ…        | 3            | Tokens available     |
| 0.4s          | 3         | 3               | âœ…        | 2            | Tokens available     |
| 0.6s          | 4         | 2               | âœ…        | 1            | Tokens available     |
| 0.8s          | 5         | 1               | âœ…        | 0            | Tokens available     |
| 1.0s          | 6         | 0 â†’ +1 (refill) | âœ…        | 0            | Refill happens at 1s |
| 1.2s          | 7         | 0               | âŒ        | 0            | No token yet         |
| 1.4s          | 8         | 0               | âŒ        | 0            | No token             |
| 1.6s          | 9         | 0               | âŒ        | 0            | No token             |
| 1.8s          | 10        | 0               | âŒ        | 0            | No token             |

Thatâ€™s why output shows **6 allowed** and **4 denied**.

---

## ğŸ§­ Summary

| Concept                 | Meaning                                |
| ----------------------- | -------------------------------------- |
| **Bucket capacity**     | `rateLimit` (5 tokens)                 |
| **Refill rate**         | 1 token every `refillTime`             |
| **Token channel**       | Represents available â€œpermitsâ€         |
| **Non-blocking select** | Prevents waiting on empty/full channel |
| **Refill goroutine**    | Keeps bucket topped up periodically    |

---

## ğŸ§© Real-World Analogy

Imagine a **parking lot** with 5 spaces:

* Each car (request) takes a space (token).
* Once full â†’ new cars wait outside (denied).
* Every few seconds, one car leaves (refill), freeing up a spot.

---

# 2ï¸âƒ£ **FIXED WINDOW ALGORITHM** ğŸªŸ

```go 
package main

import (
	"fmt"
	"sync"
	"time"
)

// "Fixed Window Counter" implementation âš¡
// Why mutex? - So that we can protect our data when we're modifying it (locking/unlocking the critical section).

type RateLimiter struct {
	mu sync.Mutex
	count int
	limit int
	window time.Duration
	resetTime time.Time
}

func NewRateLimiter(limit int, window time.Duration) *RateLimiter{
	return &RateLimiter{
		limit: limit,
		window: window,
	}
}

func (rl *RateLimiter) Allow()bool{
	rl.mu.Lock()
	defer rl.mu.Unlock()
	now:=time.Now()

	if now.After(rl.resetTime){
		rl.resetTime= now.Add(rl.window)
		rl.count = 0
	}

	if rl.count<rl.limit{
		rl.count++
		return true
	}
	return false
}

func main() {
	rateLimiter:= NewRateLimiter(5,2*time.Second) // 5 requests

	for range 10{
		if rateLimiter.Allow(){
			fmt.Println("Request Allowed âœ…")
		}else{
			fmt.Println("Request Denied âŒ")
		}
		time.Sleep(200 * time.Millisecond) // some delay
	}

}

// OUTPUT:
// $ go run .
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Allowed âœ…
// Request Denied âŒ
// Request Denied âŒ
// Request Denied âŒ
// Request Denied âŒ
// Request Denied âŒ
```
Letâ€™s **deep-dive into our Fixed Window Counter Go code** step by step.

---

# ğŸ§  **Overview: Fixed Window Counter Algorithm**

This code implements a **Fixed Window Rate Limiter**:

* Limits the number of requests that can occur in a **fixed time window**.
* Uses a **counter (`count`)** to track requests in the current window.
* Uses a **mutex** to prevent data races when accessed concurrently.
* If the counter exceeds the limit â†’ requests are denied until the window resets.

---

# âš™ï¸ **Code Walkthrough**

### 1ï¸âƒ£ Struct Definition

```go
type RateLimiter struct {
	mu sync.Mutex
	count int
	limit int
	window time.Duration
	resetTime time.Time
}
```

* `mu sync.Mutex` â†’ ensures **thread-safe access** to `count` and `resetTime`.
* `count` â†’ tracks how many requests have been made in the **current window**.
* `limit` â†’ max requests allowed in one window.
* `window` â†’ duration of the window (e.g., 2 seconds).
* `resetTime` â†’ timestamp when the **current window ends**.

---

### 2ï¸âƒ£ Constructor Function

```go
func NewRateLimiter(limit int, window time.Duration) *RateLimiter{
	return &RateLimiter{
		limit: limit,
		window: window,
	}
}
```

* Initializes a new rate limiter with a **request limit** and **window duration**.
* `resetTime` and `count` start at zero â€” window will be initialized on the first request.

---

### 3ï¸âƒ£ Core Logic â€” `Allow()`

```go
func (rl *RateLimiter) Allow() bool {
	rl.mu.Lock()
	defer rl.mu.Unlock()
	now := time.Now()

	if now.After(rl.resetTime) {
		rl.resetTime = now.Add(rl.window)
		rl.count = 0
	}

	if rl.count < rl.limit {
		rl.count++
		return true
	}
	return false
}
```

**Step-by-step:**

1. Lock the mutex â†’ critical section (ensures concurrent safety).
2. Get current time `now`.
3. **Check if window expired**:

   * If `now` is after `resetTime`, **reset the counter** to 0.
   * Update `resetTime` to `now + window`.
   * âœ… This starts a new fixed window.
4. **Check the limit**:

   * If `count < limit` â†’ increment count, allow request âœ…
   * Else â†’ deny request âŒ

---

### 4ï¸âƒ£ Main Function

```go
func main() {
	rateLimiter := NewRateLimiter(5, 2*time.Second) // 5 requests per 2s

	for range 10 {
		if rateLimiter.Allow() {
			fmt.Println("Request Allowed âœ…")
		} else {
			fmt.Println("Request Denied âŒ")
		}
		time.Sleep(200 * time.Millisecond)
	}
}
```

**Explanation:**

* Limit = 5 requests per 2 seconds
* Requests sent every 200ms (0.2s)
* Total 10 requests

---

# ğŸ•’ **Execution Timeline**

| Request # | Time (approx) | Window | Count Before | Allowed? | Count After | Notes         |
| --------- | ------------- | ------ | ------------ | -------- | ----------- | ------------- |
| 1         | 0.0s          | 0â€“2s   | 0            | âœ…        | 1           | First request |
| 2         | 0.2s          | 0â€“2s   | 1            | âœ…        | 2           | Within limit  |
| 3         | 0.4s          | 0â€“2s   | 2            | âœ…        | 3           | Within limit  |
| 4         | 0.6s          | 0â€“2s   | 3            | âœ…        | 4           | Within limit  |
| 5         | 0.8s          | 0â€“2s   | 4            | âœ…        | 5           | Reaches limit |
| 6         | 1.0s          | 0â€“2s   | 5            | âŒ        | 5           | Limit reached |
| 7         | 1.2s          | 0â€“2s   | 5            | âŒ        | 5           | Denied        |
| 8         | 1.4s          | 0â€“2s   | 5            | âŒ        | 5           | Denied        |
| 9         | 1.6s          | 0â€“2s   | 5            | âŒ        | 5           | Denied        |
| 10        | 1.8s          | 0â€“2s   | 5            | âŒ        | 5           | Denied        |

âœ… Output matches our observed results:

```
Request Allowed âœ…
Request Allowed âœ…
Request Allowed âœ…
Request Allowed âœ…
Request Allowed âœ…
Request Denied âŒ
Request Denied âŒ
Request Denied âŒ
Request Denied âŒ
Request Denied âŒ
```

---

# ğŸ§  **Key Concepts Illustrated**

1. **Fixed Window Counter**:

   * Simple counter + time window.
   * Resets at the **end of each window**.

2. **Mutex (`sync.Mutex`)**:

   * Protects `count` and `resetTime` from **data races** if multiple goroutines call `Allow()`.

3. **Burstiness Issue**:

   * All 5 allowed requests can happen at the **end of one window**, and 5 more at the **start of the next window**, causing **bursts**.
   * This is a known limitation of the fixed window algorithm.

4. **Non-blocking**:

   * `Allow()` returns **true/false immediately**.
   * No queuing or waiting.

---

# ğŸ§© **Summary Table**

| Feature           | Implementation in this code               |
| ----------------- | ----------------------------------------- |
| Window Type       | Fixed window                              |
| Counter           | `count` tracks requests in current window |
| Limit             | `limit`                                   |
| Window Size       | `window`                                  |
| Reset             | `resetTime` checked on each `Allow()`     |
| Concurrency Safe? | Yes, via `sync.Mutex`                     |
| Drawback          | Can allow burst at window boundaries      |

---

This is the **classic fixed-window rate limiter** â€” very simple and easy to implement, but not perfect for smoothing traffic.

---

Letâ€™s visualize our **Fixed Window Rate Limiter** with a timeline diagram. This will make the allowed/denied requests much clearer.

---

# â±ï¸ **Timeline Diagram â€” Fixed Window**

**Settings from our code:**

* Limit = 5 requests per 2 seconds
* Requests every 0.2s
* Total requests = 10

```
Time (s): 0   0.2   0.4   0.6   0.8   1.0   1.2   1.4   1.6   1.8
Request #: 1    2     3     4     5     6     7     8     9     10

Window 0â€“2s:
Allowed:  âœ…   âœ…    âœ…    âœ…    âœ…  
Denied:                                     âŒ    âŒ    âŒ    âŒ    âŒ

Window resets at 2s:
Next allowed requests would start from count = 0
```

---

### ğŸŸ¢ **Explanation**

1. **Window 0â€“2s**

   * Requests 1â€“5 â†’ counter < limit â†’ **allowed** âœ…
   * Requests 6â€“10 â†’ counter = limit â†’ **denied** âŒ

2. **After 2s**

   * `resetTime` updates â†’ `count` resets to 0
   * Next batch of requests would again be allowed up to the limit

---

### ğŸ”¹ **Visual Representation**

```
[Window 0-2s]  
Req#1 âœ…  Req#2 âœ…  Req#3 âœ…  Req#4 âœ…  Req#5 âœ…  Req#6 âŒ  Req#7 âŒ  Req#8 âŒ  Req#9 âŒ  Req#10 âŒ
```

**Key Observations:**

* **All allowed requests happen first**, then all denials â€” classic **fixed-window burstiness**.
* If we had requests **straddling two windows**, the limiter would reset exactly at `2s` and start allowing again.

---

### âš¡ **Burstiness Problem (Optional Note)**

If requests were made like this:

```
Request 1-5: at 1.9s â†’ allowed (end of previous window)
Request 6-10: at 2.1s â†’ allowed (start of new window)
```

Then **10 requests could happen in just 0.2s**, even though limit = 5 per 2s.

This is why **Sliding Window or Token Bucket** is preferred for smoother control.

---
![alt text](image.png)

# 3ï¸âƒ£ **LEAKY-BUCKET ALGORITHM** ğŸ’¦
---

## ğŸš° **Leaky Bucket Algorithm â€” Concept**

| ğŸ§  Concept  | Description                                                                                                                                                                                                         |
| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Idea**    | Think of a bucket with a small hole at the bottom â€” water (requests) drips out at a constant rate. If too much water (traffic) is poured in too fast, the bucket overflows, and extra water (requests) are dropped. |
| **Purpose** | Smooths out **bursty traffic** into a steady outflow rate.                                                                                                                                                          |
| **Analogy** | Requests â€œleakâ€ out of the bucket (are processed) at a constant rate. The bucket has limited capacity â€” if itâ€™s full, new requests are denied.                                                                      |
| **Type**    | Queue-based rate limiter with constant drain rate.                                                                                                                                                                  |

---

## âš™ï¸ **Golang Implementation**

```go
package main

import (
	"fmt"
	"sync"
	"time"
)

// "Leaky Bucket" Algorithm âš¡
// Limits burst traffic by draining requests at a constant rate.

type LeakyBucket struct {
	mu        sync.Mutex
	capacity  int           // max requests that can wait in the bucket
	interval  time.Duration // leak rate (time between processing)
	queue     int           // current number of requests in bucket
}

func NewLeakyBucket(capacity int, interval time.Duration) *LeakyBucket {
	lb := &LeakyBucket{
		capacity: capacity,
		interval: interval,
	}
	// Constant leaking (draining) process
	go lb.startLeaking()
	return lb
}

func (lb *LeakyBucket) startLeaking() {
	ticker := time.NewTicker(lb.interval)
	defer ticker.Stop()
	for {
		<-ticker.C
		lb.mu.Lock()
		if lb.queue > 0 {
			lb.queue-- // process (leak) one request
		}
		lb.mu.Unlock()
	}
}

func (lb *LeakyBucket) Allow() bool {
	lb.mu.Lock()
	defer lb.mu.Unlock()

	if lb.queue < lb.capacity {
		lb.queue++ // add request to bucket
		return true
	}
	return false // bucket full, drop request
}

func main() {
	bucket := NewLeakyBucket(5, 500*time.Millisecond) // capacity=5, leak rate=1 req/0.5s

	for i := 1; i <= 10; i++ {
		if bucket.Allow() {
			fmt.Printf("Request %d allowed âœ…\n", i)
		} else {
			fmt.Printf("Request %d denied âŒ (bucket full)\n", i)
		}
		time.Sleep(200 * time.Millisecond) // incoming traffic rate
	}
}
```

---

## ğŸ§© **How It Works**

| Step | Description                                                             |
| ---- | ----------------------------------------------------------------------- |
| 1ï¸âƒ£  | We create a bucket that can hold **5 requests max**.                    |
| 2ï¸âƒ£  | Each **500ms**, the bucket â€œleaksâ€ (processes) **1 request**.           |
| 3ï¸âƒ£  | If requests arrive faster than the leak rate, the bucket fills up.      |
| 4ï¸âƒ£  | Once full, extra requests are **denied** until space becomes available. |

---

## ğŸ§® **Timeline Example**

| Time (s) | Request # | Action    | Bucket Size | Reason           |
| -------- | --------- | --------- | ----------- | ---------------- |
| 0.0      | 1         | âœ… Allowed | 1           | Bucket empty     |
| 0.2      | 2         | âœ… Allowed | 2           |                  |
| 0.4      | 3         | âœ… Allowed | 3           |                  |
| 0.5      | Leak      | â€”         | 2           | 1 request leaked |
| 0.6      | 4         | âœ… Allowed | 3           |                  |
| 0.8      | 5         | âœ… Allowed | 4           |                  |
| 1.0      | Leak      | â€”         | 3           |                  |
| 1.0      | 6         | âœ… Allowed | 4           |                  |
| 1.2      | 7         | âœ… Allowed | 5           | Bucket full      |
| 1.4      | 8         | âŒ Denied  | 5           | Overflow         |
| 1.6      | Leak      | â€”         | 4           | Space freed      |
| 1.6      | 9         | âœ… Allowed | 5           |                  |
| 1.8      | 10        | âŒ Denied  | 5           | Still full       |

---

## ğŸ” **Characteristics**

| Property              | Description                                            |
| --------------------- | ------------------------------------------------------ |
| **Smooth Output**     | Requests processed at constant pace.                   |
| **Burst Handling**    | Bursts tolerated up to `capacity` limit.               |
| **Memory Efficiency** | Uses simple counters and timers.                       |
| **Concurrency-Safe**  | Mutex ensures no race condition when modifying bucket. |

---

## ğŸ§  **Difference from Token Bucket**

| Feature        | Token Bucket               | Leaky Bucket                                          |
| -------------- | -------------------------- | ----------------------------------------------------- |
| Token Flow     | Tokens are added over time | Requests leak over time                               |
| Burst Handling | Allows short bursts        | Smooths bursts                                        |
| Ideal Use      | APIs where bursts are OK   | Queues or message systems where steady rate is needed |

---

## ğŸ”§ **Real-World Analogy**

> Imagine customer calls coming into a support center (bucket).
> The agent (drain) handles one call every 30 seconds (leak rate).
> If too many customers call at once, only 5 can wait â€” the rest hear â€œall lines are busyâ€ (denied).

---


![alt text](image-1.png)

```go
package main

// LEAKY BUCKET ALGO. ğŸ’¦

import (
	"fmt"
	"sync"
	"time"
)

type LeakyBucket struct {
	capacity int
	leakRate time.Duration
	tokens int
	lastLeak time.Time
	mu sync.Mutex
}

func NewLeakyBucket(capacity int, leakRate time.Duration) *LeakyBucket{
	return &LeakyBucket{
		capacity: capacity,
		leakRate: leakRate,
		tokens: capacity,
		lastLeak: time.Now(),
	}
}

func (lb *LeakyBucket) Allow()bool{
	lb.mu.Lock()
	defer lb.mu.Unlock()
	now:= time.Now()
	elapsedTime:= now.Sub(lb.lastLeak)
	tokensToAdd:= int(elapsedTime/lb.leakRate)
	lb.tokens += tokensToAdd

	if lb.tokens > lb.capacity{
		lb.tokens=lb.capacity
	}
	lb.lastLeak = lb.lastLeak.Add(time.Duration(tokensToAdd) * lb.leakRate)

	if lb.tokens > 0{
		lb.tokens--
		return true
	}
	return false
}

func main() {
	leakyBucket:= NewLeakyBucket(5,500*time.Millisecond)

	for range 10{
		if leakyBucket.Allow(){
			fmt.Println("Request Accepted âœ…")
		}else{
			fmt.Println("Request Rejected âŒ")
		}
		time.Sleep(200*time.Millisecond)
	}
}

// O:P
// go run .
// Request Accepted âœ…
// Request Accepted âœ…
// Request Accepted âœ…
// Request Accepted âœ…
// Request Accepted âœ…
// Request Accepted âœ…
// Request Accepted âœ…
// Request Rejected âŒ
// Request Accepted âœ…
// Request Rejected âŒ
```

ğŸ§  Letâ€™s break down this **Leaky Bucket Algorithm** implementation in Go â€” line by line and concept by concept â€” to fully understand how it works internally.

---

## ğŸ’§ Overview: Whatâ€™s a Leaky Bucket?

The **Leaky Bucket algorithm** is a **rate-limiting technique** that smooths out bursts by processing requests at a **fixed, constant rate** â€” just like water leaking steadily from a bucket.

* The **bucket** = a fixed-capacity container holding â€œtokensâ€ (representing available requests).
* The **leak rate** = how quickly tokens are drained (processed).
* When the bucket is **full**, new requests overflow (get rejected).

This creates a **constant output rate** â€” even if incoming requests are bursty.

---

## ğŸ§± Code Walkthrough

### ğŸ§© Struct Definition

```go
type LeakyBucket struct {
	capacity  int           // Maximum number of requests allowed (bucket size)
	leakRate  time.Duration // Time it takes for one token to leak out
	tokens    int           // Current tokens (representing available capacity)
	lastLeak  time.Time     // Timestamp of the last leak
	mu        sync.Mutex    // Ensures thread-safe access (important for concurrency)
}
```

* **capacity:** The bucketâ€™s max limit â€” how many requests can be queued at once.
* **leakRate:** Determines how often tokens leak out (processed).
  Example: `500ms` â†’ one token leaks every half second.
* **tokens:** Tracks how many tokens currently in the bucket.
* **lastLeak:** When was the bucket last drained.
* **mutex:** To prevent **race conditions** when multiple goroutines call `Allow()` simultaneously.

---

### ğŸ§© Constructor Function

```go
func NewLeakyBucket(capacity int, leakRate time.Duration) *LeakyBucket {
	return &LeakyBucket{
		capacity: capacity,
		leakRate: leakRate,
		tokens: capacity,
		lastLeak: time.Now(),
	}
}
```

* Initializes a **new bucket**.
* Starts **fully filled** (`tokens = capacity`).
* Sets the initial `lastLeak` timestamp to the current time.

âœ… The bucket is full at the start, so initial requests can be accepted immediately.

---

### ğŸ§© Core Logic â€” `Allow()` Function

```go
func (lb *LeakyBucket) Allow() bool {
	lb.mu.Lock()
	defer lb.mu.Unlock()
```

Locks the bucket to ensure only one request modifies its state at a time (avoids concurrent access bugs).

---

#### ğŸ•’ Step 1: Calculate Time Elapsed Since Last Leak

```go
	now := time.Now()
	elapsedTime := now.Sub(lb.lastLeak)
```

* `elapsedTime` = how long since last leak happened.
* Weâ€™ll use this to determine how many tokens should have leaked out during that time.

---

#### ğŸ§® Step 2: Determine Tokens to Refill

```go
	tokensToAdd := int(elapsedTime / lb.leakRate)
	lb.tokens += tokensToAdd
```

Example:
If `leakRate = 500ms` and `elapsedTime = 1500ms`,
â†’ `tokensToAdd = 3` (because 3 Ã— 500ms fits into 1500ms).

We â€œrefillâ€ by adding these tokens back, meaning some requests have been processed (leaked out), freeing capacity.

---

#### ğŸ§¯ Step 3: Cap the Tokens at Capacity

```go
	if lb.tokens > lb.capacity {
		lb.tokens = lb.capacity
	}
```

Prevents tokens from exceeding the bucketâ€™s max size.
This maintains a consistent upper bound â€” **bucket overflow is not allowed.**

---

#### ğŸ•’ Step 4: Update `lastLeak`

```go
	lb.lastLeak = lb.lastLeak.Add(time.Duration(tokensToAdd) * lb.leakRate)
```

Moves `lastLeak` forward based on how much time we accounted for during refill.
If we added 2 tokens, that means 2 leak intervals have passed â€” so we advance `lastLeak` accordingly.

---

#### ğŸš¦ Step 5: Check If Request Allowed

```go
	if lb.tokens > 0 {
		lb.tokens--           // Consume one token
		return true           // Request allowed âœ…
	}
	return false              // Otherwise, bucket empty â†’ reject âŒ
```

If at least one token available, we **consume** it (decrement), representing one accepted request.
Otherwise, we **reject** because the bucket has no room left.

---

### ğŸ§© Main Function

```go
func main() {
	leakyBucket := NewLeakyBucket(5, 500*time.Millisecond)

	for range 10 {
		if leakyBucket.Allow() {
			fmt.Println("Request Accepted âœ…")
		} else {
			fmt.Println("Request Rejected âŒ")
		}
		time.Sleep(200 * time.Millisecond)
	}
}
```

* We create a bucket with:

  * **Capacity = 5**
  * **Leak rate = 1 token per 500ms**
* Then send **10 requests**, each **200ms apart**.

---

## ğŸ§© Execution Breakdown (Timeline Simulation)

| Time (s) | Action       | Tokens Left | Explanation                           |
| -------- | ------------ | ----------- | ------------------------------------- |
| 0.0      | Request 1 âœ…  | 4           | Initially full (5)                    |
| 0.2      | Request 2 âœ…  | 3           | 200ms later                           |
| 0.4      | Request 3 âœ…  | 2           | 200ms later                           |
| 0.6      | Request 4 âœ…  | 1           | 200ms later                           |
| 0.8      | Request 5 âœ…  | 0           | Bucket empty now                      |
| 1.0      | Request 6 âœ…  | 0           | One token leaked (since 500ms passed) |
| 1.2      | Request 7 âœ…  | 0           | Another leak just before request      |
| 1.4      | Request 8 âŒ  | 0           | No leak yet â†’ rejected                |
| 1.6      | Request 9 âœ…  | 0           | Leak replenished one token            |
| 1.8      | Request 10 âŒ | 0           | No leak â†’ rejected                    |

---

## ğŸ§  Key Takeaways

| Concept                | Explanation                                                        |
| ---------------------- | ------------------------------------------------------------------ |
| **Leak rate**          | Controls how fast requests are processed (steady outflow).         |
| **Capacity**           | Defines how many requests can queue before rejecting new ones.     |
| **Thread-safety**      | Achieved using `sync.Mutex`.                                       |
| **Tokens**             | Represent how many requests can be accepted immediately.           |
| **Leaking simulation** | Done by calculating elapsed time and restoring tokens accordingly. |

---

## ğŸ§° Real-World Analogy

Imagine a **funnel** with a small hole at the bottom:

* Water (requests) can pour in, but only leaks out at a fixed rate.
* If we pour too fast (too many requests), the funnel overflows (rejects).

---

## âœ… Advantages

* Produces **consistent output rate** (no bursts).
* Easy to reason about timing.
* Great for **scheduling jobs** or **limiting worker throughput**.

---

## âŒ Disadvantages

* Doesnâ€™t allow short bursts (unlike Token Bucket).
* If idle for a while, it doesnâ€™t accumulate â€œcredit.â€
* Slightly complex time-based math.

---

## ğŸ§¾ Summary Table

| Property           | Value                             |
| ------------------ | --------------------------------- |
| **Algorithm Type** | Leaky Bucket                      |
| **Behavior**       | Smooth constant leak rate         |
| **Allows Burst?**  | âŒ No                              |
| **Thread-safe?**   | âœ… Yes (mutex protected)           |
| **Main Use Case**  | Job schedulers, background queues |
| **Core Variable**  | `tokens`, `leakRate`, `lastLeak`  |

---

**Final Insight:**
This code demonstrates a **classic leaky bucket rate limiter** â€”
It enforces a **steady throughput** regardless of how fast requests arrive.
Itâ€™s ideal when we care about **predictable processing rate**, not bursts â€” such as in **worker queues, message consumers, or network shaping.**

---

# **LEAKY-BUCKET ALGO. + GOROUTINES** â³
```go
package main

// LEAKY BUCKET ALGO. + GOROUTINES ğŸ’¦

import (
	"fmt"
	"sync"
	"time"
)

type LeakyBucket struct {
	capacity int
	leakRate time.Duration
	tokens int
	lastLeak time.Time
	mu sync.Mutex
}

func NewLeakyBucket(capacity int, leakRate time.Duration) *LeakyBucket{
	return &LeakyBucket{
		capacity: capacity,
		leakRate: leakRate,
		tokens: capacity,
		lastLeak: time.Now(),
	}
}

func (lb *LeakyBucket) Allow()bool{
	lb.mu.Lock()
	defer lb.mu.Unlock()
	now:= time.Now()
	elapsedTime:= now.Sub(lb.lastLeak)
	tokensToAdd:= int(elapsedTime/lb.leakRate)
	lb.tokens += tokensToAdd

	if lb.tokens > lb.capacity{
		lb.tokens=lb.capacity
	}
	lb.lastLeak = lb.lastLeak.Add(time.Duration(tokensToAdd) * lb.leakRate)


	fmt.Printf("ğŸ”¥ Tokens added %d, Tokens subtracted %d, Total Tokens %d\n", tokensToAdd, 1, lb.tokens)
	fmt.Printf("ğŸ•°ï¸ Last leak-time: %v", lb.lastLeak)

	if lb.tokens > 0{
		lb.tokens--
		return true
	}
	return false
}

func main() {
	leakyBucket:= NewLeakyBucket(5,500*time.Millisecond)
	var wg sync.WaitGroup

	for range 10{
		wg.Add(1)
		go func() {
			defer wg.Done()
			if leakyBucket.Allow(){
			fmt.Println("ğŸ•› Curr. time:",time.Now())
			fmt.Println("Request Accepted âœ…")
		}else{
			fmt.Println("ğŸ•› Curr. time:",time.Now())
			fmt.Println("Request Rejected âŒ")
		}
		time.Sleep(200*time.Millisecond)
		}()
		
	}

	time.Sleep(500*time.Millisecond)
	wg.Wait()
}

// O:P -
// go run .
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 5
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6825644 +0530 IST m=+0.002029101
// Request Accepted âœ…
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 4
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6836974 +0530 IST m=+0.003162101
// Request Accepted âœ…
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 3
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.684388 +0530 IST m=+0.003852701
// Request Accepted âœ…
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 2
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6857004 +0530 IST m=+0.005165101
// Request Accepted âœ…
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 1
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6857004 +0530 IST m=+0.005165101
// Request Accepted âœ…
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 0
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=+0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6862702 +0530 IST m=+0.005734901
// Request Rejected âŒ
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 0      
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=++0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6867999 +0530 IST m=+0.006264601
// Request Rejected âŒ
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 0      
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=++0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6867999 +0530 IST m=+0.006264601
// Request Rejected âŒ
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 0      
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=++0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6867999 +0530 IST m=+0.006264601
// Request Rejected âŒ
// ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 0      
// ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST m=++0.000745601ğŸ•› Curr. time: 2025-10-11 12:25:42.6873807 +0530 IST m=+0.006845401
// Request Rejected âŒ
```

Perfect â€” this is an excellent evolution of the **Leaky Bucket Algorithm** with **Goroutines**, which now simulates *real concurrent incoming requests*, just like what happens in an actual web server or API gateway.

Letâ€™s break it down step by step â€” every line, every concept, including how the output behaves the way it does. âš™ï¸ğŸ’¦

---

## ğŸ§  Concept Refresher

The **Leaky Bucket Algorithm** limits requests to a *steady leak rate*, even if many requests arrive simultaneously.
Imagine a **bucket with a hole** â€” water (requests) can be poured in, but leaks out at a fixed rate.
If the bucket is full, excess water spills (requests are rejected).

This version adds **goroutines** to mimic *concurrent request arrivals* â€” a real-world scenario in servers.

---

## ğŸ§© Struct Definition

```go
type LeakyBucket struct {
	capacity  int           // Maximum tokens (bucket size)
	leakRate  time.Duration // Rate at which tokens leak (refill frequency)
	tokens    int           // Current available tokens
	lastLeak  time.Time     // Timestamp of the last leak event
	mu        sync.Mutex    // Protects shared data across goroutines
}
```

âœ… Each field plays a vital role:

* `capacity`: max burst limit (how many requests we can handle at once).
* `leakRate`: defines how often one token leaks out.
* `tokens`: current count (remaining capacity in bucket).
* `lastLeak`: when tokens were last updated.
* `mu`: ensures **thread safety** across multiple goroutines calling `Allow()` at once.

---

## âš™ï¸ Initialization

```go
func NewLeakyBucket(capacity int, leakRate time.Duration) *LeakyBucket {
	return &LeakyBucket{
		capacity: capacity,
		leakRate: leakRate,
		tokens: capacity,        // Start full
		lastLeak: time.Now(),    // Current timestamp
	}
}
```

At startup:

* Bucket is **full** (ready to accept requests).
* Leak timer starts now.

So we start with 5 available tokens.

---

## ğŸ”’ Core Function â€” `Allow()`

```go
func (lb *LeakyBucket) Allow() bool {
	lb.mu.Lock()
	defer lb.mu.Unlock()
```

Locks access so that no two goroutines modify the bucket at the same time.
Without this mutex, weâ€™d get **race conditions** and wrong token counts.

---

### ğŸ•’ Step 1 â€” Calculate Elapsed Time

```go
now := time.Now()
elapsedTime := now.Sub(lb.lastLeak)
```

Determines how much time has passed since the last leak.
This helps compute how many tokens have â€œleaked outâ€ during that interval.

---

### ğŸ§® Step 2 â€” Calculate Tokens to Add (Refill)

```go
tokensToAdd := int(elapsedTime / lb.leakRate)
lb.tokens += tokensToAdd
```

If:

* `leakRate = 500ms`
* `elapsedTime = 2s`

then `tokensToAdd = 4` (since 4 Ã— 500ms fits into 2s).
We refill the bucket with up to 4 tokens.

---

### ğŸš« Step 3 â€” Prevent Overflow

```go
if lb.tokens > lb.capacity {
	lb.tokens = lb.capacity
}
```

Ensures tokens never exceed the bucketâ€™s capacity.

---

### ğŸ•°ï¸ Step 4 â€” Update Last Leak Time

```go
lb.lastLeak = lb.lastLeak.Add(time.Duration(tokensToAdd) * lb.leakRate)
```

Moves the `lastLeak` forward by however many leaks occurred.
If 2 tokens leaked, then `lastLeak` moves forward by `1s`.

---

### ğŸ“Š Step 5 â€” Debug Logging

```go
fmt.Printf("ğŸ”¥ Tokens added %d, Tokens subtracted %d, Total Tokens %d\n", tokensToAdd, 1, lb.tokens)
fmt.Printf("ğŸ•°ï¸ Last leak-time: %v", lb.lastLeak)
```

Shows internal bucket state:

* How many tokens were added due to elapsed time,
* One token subtracted for current request,
* Total remaining after that.

---

### ğŸš¦ Step 6 â€” Decide Request Fate

```go
if lb.tokens > 0 {
	lb.tokens--
	return true
}
return false
```

* If tokens are available â†’ **Accept request** âœ…
* Otherwise â†’ **Reject request** âŒ

Each accepted request consumes one token.

---

## ğŸ§© The Main Function

```go
func main() {
	leakyBucket := NewLeakyBucket(5, 500*time.Millisecond)
	var wg sync.WaitGroup
```

We create:

* A **bucket** with `capacity = 5` and `leakRate = 500ms`
* A `WaitGroup` to ensure all goroutines complete.

---

### ğŸ§µ Launch 10 Concurrent Requests

```go
for range 10 {
	wg.Add(1)
	go func() {
		defer wg.Done()
		if leakyBucket.Allow() {
			fmt.Println("ğŸ•› Curr. time:", time.Now())
			fmt.Println("Request Accepted âœ…")
		} else {
			fmt.Println("ğŸ•› Curr. time:", time.Now())
			fmt.Println("Request Rejected âŒ")
		}
		time.Sleep(200 * time.Millisecond)
	}()
}
```

Each goroutine simulates a **client request** hitting the API.

Because all goroutines start *almost simultaneously*, they **compete** for the 5 available tokens instantly.

Hence, at the beginning:

* 5 requests are accepted âœ…
* 5 requests are rejected âŒ (bucket empty)

---

### ğŸ’¤ Final Cleanup

```go
time.Sleep(500 * time.Millisecond)
wg.Wait()
```

Gives time for leaks/refills and waits for all goroutines to finish execution.

---

## ğŸ§¾ Sample Output Breakdown

```
ğŸ”¥ Tokens added 0, Tokens subtracted 1, Total Tokens 5
ğŸ•°ï¸ Last leak-time: 2025-10-11 12:25:42.6812809 +0530 IST
ğŸ•› Curr. time: ... Request Accepted âœ…
```

Letâ€™s interpret:

| Line                | Meaning                                                                            |
| ------------------- | ---------------------------------------------------------------------------------- |
| ğŸ”¥ Tokens added 0   | No time elapsed since last leak, so no new tokens refilled yet                     |
| Tokens subtracted 1 | Current request consumes one token                                                 |
| Total Tokens 5 â†’ 4  | Bucket shrinks as requests are accepted                                            |
| ğŸ•°ï¸ Last leak-time  | Remains same for all concurrent calls because almost zero time passed between them |

Thatâ€™s why in all initial requests, `tokensToAdd = 0`.

When `tokens` hit 0, all subsequent requests instantly see an empty bucket and are **rejected**.

---

## ğŸ’¡ Why Are All â€œLast leak-timeâ€ Values Identical?

Because all 10 goroutines started *within microseconds* of each other â€”
no measurable 500ms interval passed between them, so **no leaks occurred yet**.

So:

* All see the same `elapsedTime â‰ˆ 0`
* Hence, `tokensToAdd = 0`
* Once `tokens` reach 0 â†’ rejections begin

---

## ğŸ§  Conceptual Summary

| Concept             | Description                                                            |
| ------------------- | ---------------------------------------------------------------------- |
| **Concurrency**     | Multiple goroutines simulate parallel incoming requests                |
| **Mutex Locking**   | Ensures atomic access to `tokens` across goroutines                    |
| **Capacity**        | Max concurrent requests allowed before rejections                      |
| **Leak Rate**       | Defines steady request processing rate (500ms per leak)                |
| **lastLeak**        | Controls token refill timing                                           |
| **Output Behavior** | Burst of accepted requests followed by denials, until leak replenishes |

---

## ğŸ§¾ Comparison with Sequential Version

| Feature         | Sequential Version | Goroutine Version             |
| --------------- | ------------------ | ----------------------------- |
| Requests arrive | One by one         | Concurrently (parallel)       |
| Refill visible  | Yes (with sleep)   | No (too fast for leaks)       |
| Output order    | Predictable        | Unordered (due to goroutines) |
| Realism         | Simulated client   | Closer to real-world servers  |

---

## âœ… TL;DR â€” What Happens

1. 10 requests arrive *at the same time* via goroutines.
2. 5 get accepted immediately (initial capacity).
3. Remaining 5 see no available tokens â†’ rejected.
4. Leak/refill didnâ€™t occur yet because 500ms hadnâ€™t passed.
5. Once leak interval passes, tokens would slowly refill for new requests.

---


