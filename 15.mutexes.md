1ï¸âƒ£ **MUTEX EXECUTION** âš¡

Letâ€™s walk through **the following code**, **line by line**..

---

```go 
package main

import (
	"fmt"
	"sync"
)

/*
ğŸ’¡mutexes - mitual exclution, sync. primitive which prevents multiple goroutines from simultaneously accessing shared resources or exeuting critical sections of the code. Ensures that only 1 goroutine can hold the 'mutex' at a time, thus avaoiding race-conditions and data-corruption. ğŸ’»
*/

type Counter struct{
	mu sync.Mutex
	val int
}

func (c *Counter)increment(){
	c.mu.Lock()
	c.val++
	defer c.mu.Unlock()
}

func (c *Counter) getVal()int{
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.val
}

func main() {
	// waitgroups for multiple goroutines
	var wg sync.WaitGroup
	counter:= &Counter{}
	numOfGoroutines := 10

	for range numOfGoroutines{
		wg.Add(1)
		go func() {
			defer wg.Done()
			for range 100{
				counter.increment()
			}
		}()
	}
	wg.Wait()
	fmt.Printf("âœ… Final counter val: %d\n",counter.getVal()) // âš ï¸unreliable output without 'mutex'

	// O.P- 
	// $ go run .
	// âœ… Final counter val: 1000
}
```

### ğŸ§  Step-by-step Breakdown

---

#### **Imports**

```go
import (
	"fmt"
	"sync"
)
```

Weâ€™re importing:

* `fmt` â†’ for printing output to the console.
* `sync` â†’ which gives us synchronization primitives like `Mutex` and `WaitGroup` for managing concurrency safely.

---

#### **Comment Block**

```go
/*
mutexes - mitual exclution, sync. primitive which prevents multiple goroutines from simultaneously accessing shared resources or exeuting critical sections of the code. Ensures that only 1 goroutine can hold the 'mutex' at a time, thus avaoiding race-conditions and data-corruption. ğŸ’»
*/
```

Thatâ€™s a perfect conceptual description already:
ğŸ‘‰ A **mutex (mutual exclusion lock)** ensures that only **one goroutine** can access a critical section (shared resource) at any given time.
This prevents **race conditions** â€” situations where multiple goroutines try to read and write to the same variable concurrently, leading to unpredictable results.

---

#### **Struct Definition**

```go
type Counter struct{
	mu sync.Mutex
	val int
}
```

We define a struct named `Counter` that contains:

* `mu`: a **mutex** to control concurrent access.
* `val`: an integer counter we want to update safely from multiple goroutines.

Think of the mutex as a **lock on a door** â€” only one goroutine can enter (modify `val`) at a time.

---

#### **increment() method**

```go
func (c *Counter)increment(){
	c.mu.Lock()
	c.val++
	defer c.mu.Unlock()
}
```

This method increments the counter.

Letâ€™s go line-by-line inside it:

1. **`c.mu.Lock()`**

   * This locks the mutex.
   * If another goroutine already holds the lock, this goroutine will **wait (block)** until the mutex is unlocked.
   * Once it obtains the lock, it enters the critical section (safe area to modify `val`).

2. **`c.val++`**

   * This is the critical section.
   * `val++` is *not* an atomic operation â€” it actually does three things internally:

     ```
     temp = c.val
     temp = temp + 1
     c.val = temp
     ```

     So if multiple goroutines did this at the same time without a mutex, theyâ€™d overwrite each otherâ€™s updates â€” thatâ€™s the classic race condition.

3. **`defer c.mu.Unlock()`**

   * The `defer` schedules the unlock to run *after the function finishes*.
   * In Go, deferred calls run in **LIFO order** just before the function returns.
   * So when `increment()` ends, the mutex automatically unlocks â€” allowing another goroutine to acquire it and safely increment again.

ğŸŸ¡ **Note:** Normally, we place `defer c.mu.Unlock()` *right after* `Lock()` to guarantee unlock even if something panics before the increment.
But since `c.val++` here canâ€™t panic, it still works â€” just not the most idiomatic placement.

---

#### **getVal() method**

```go
func (c *Counter) getVal()int{
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.val
}
```

This method safely reads the counterâ€™s value.

* It locks before reading `c.val` (to ensure weâ€™re not reading while another goroutine is writing).
* It defers unlock immediately (idiomatic).
* It returns the counter value once the lock is released.

So even reading is synchronized â€” preventing data races between read/write.

---

#### **main() function**

```go
func main() {
	// waitgroups for multiple goroutines
	var wg sync.WaitGroup
	counter:= &Counter{}
	numOfGoroutines := 10
```

Here:

* `wg` â†’ a `WaitGroup` to track and wait for multiple goroutines to finish.
* `counter` â†’ a pointer to a `Counter` instance (we use a pointer so we donâ€™t accidentally copy the mutex).
* `numOfGoroutines := 10` â†’ weâ€™ll spawn 10 goroutines to increment the counter concurrently.

---

#### **Launching Goroutines**

```go
	for range numOfGoroutines{
		wg.Add(1)
		go func() {
			defer wg.Done()
			for range 100{
				counter.increment()
			}
		}()
	}
```

Even though `for range numOfGoroutines` and `for range 100` wouldnâ€™t compile in Go (youâ€™d normally write `for i := 0; i < numOfGoroutines; i++`), letâ€™s **conceptually** understand what this loop is *intended* to do:

* The loop runs **10 times**, creating 10 goroutines.
* Each goroutine:

  * Calls `defer wg.Done()` â†’ meaning when that goroutine finishes, it signals the WaitGroup that one job is done.
  * Runs another loop **100 times**, calling `counter.increment()` each time.

So effectively:

```
10 goroutines Ã— 100 increments each = 1000 total increments
```

Each call to `increment()` locks the mutex, increments safely, and unlocks, one at a time.

This creates **serialized access** to the shared counter.

---

#### **Wait for Completion**

```go
	wg.Wait()
```

The main goroutine blocks here until all 10 goroutines have called `wg.Done()` (i.e., finished their 100 increments each).

Once all have finished â†’ `wg.Wait()` unblocks.

---

#### **Final Print**

```go
	fmt.Printf("âœ… Final counter val: %d\n",counter.getVal()) // unreliable output without 'mutex'
```

Finally, we print the counter value.

If mutexes were missing:

* The increments would overlap (race conditions).
* The result would be random and **less than 1000** most of the time.

But since we used the mutex correctly, the result is always:

```
âœ… Final counter val: 1000
```

---

### ğŸ” Why it works perfectly

1. Only one goroutine modifies `c.val` at any given moment.
2. The mutex ensures mutual exclusion.
3. The WaitGroup ensures synchronization â€” we donâ€™t print before work finishes.
4. The final print happens after all increments are complete.

Hence, the output is deterministic and correct.

---

### âš™ï¸ Under the Hood

Letâ€™s visualize whatâ€™s happening during execution:

```
Goroutine-1 locks -> increments -> unlocks
Goroutine-2 locks -> increments -> unlocks
Goroutine-3 locks -> increments -> unlocks
...
```

Each goroutine tries to acquire the lock. If itâ€™s taken, it waits.
Once the lock is free, it grabs it, increments, defers the unlock, and releases it â€” giving the next goroutine a chance.

This sequence happens very fast, so effectively the counter increments appear atomic.

---

### ğŸ§© Summary Table

| Concept          | Purpose                                                                    |
| ---------------- | -------------------------------------------------------------------------- |
| `sync.Mutex`     | Prevents multiple goroutines from entering critical section simultaneously |
| `Lock()`         | Acquire the mutex (waits if already locked)                                |
| `Unlock()`       | Releases the mutex                                                         |
| `defer Unlock()` | Ensures unlock always happens when function exits                          |
| `sync.WaitGroup` | Waits until all goroutines finish                                          |
| `wg.Add(n)`      | Tells how many goroutines to wait for                                      |
| `wg.Done()`      | Decrements the counter                                                     |
| `wg.Wait()`      | Blocks until counter hits zero                                             |
| `c.val++`        | Shared data safely modified under lock                                     |
| Final result     | Always `1000` (since 10 Ã— 100 increments)                                  |

---

### ğŸ§  TL;DR

Your code demonstrates **safe concurrent access** to a shared variable using:

* `sync.Mutex` â†’ to ensure **mutual exclusion**.
* `sync.WaitGroup` â†’ to ensure **completion synchronization**.

Each goroutine safely increments the counter 100 times, producing a predictable final output:

```
âœ… Final counter val: 1000
```
---

Alright letâ€™s go through **the exact code** line by line, as it is, **without changing anything**, and deeply understand whatâ€™s going on under the hood.

---

### ğŸ§± Code Overview

```go
package main

import (
	"fmt"
	"sync"
)
```

Weâ€™re importing:

* **fmt** â†’ for printing output
* **sync** â†’ for synchronization primitives like `Mutex` and `WaitGroup`

---

### ğŸ”’ Step 1: The `Counter` struct

```go
type Counter struct {
	mu  sync.Mutex
	val int
}
```

* `mu` â†’ a **mutex** that ensures mutual exclusion.
* `val` â†’ our **shared resource** (the counter value) that multiple goroutines will modify.

The mutex ensures that **only one goroutine at a time** can access/modify `val`.

---

### âš™ï¸ Step 2: `increment` method

```go
func (c *Counter) increment() {
	c.mu.Lock()
	c.val++
	defer c.mu.Unlock()
}
```

Letâ€™s unpack this carefully:

1. `c.mu.Lock()` â†’ acquires the **lock**.

   * If another goroutine already holds the lock, this goroutine will **block** (pause) until the lock is released.

2. `c.val++` â†’ increments the shared counter.

3. `defer c.mu.Unlock()` â†’ schedules unlocking once the function returns.

> ğŸ’¡ Even though the `defer` line appears **after** `c.val++`, itâ€™s still **registered immediately** when that line executes.
> So, `Unlock()` will happen when `increment()` finishes.

In effect, for each goroutine:

* It **locks**, increments safely, and **then unlocks** before another goroutine can enter this critical section.

---

### ğŸ” Step 3: `getVal` method

```go
func (c *Counter) getVal() int {
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.val
}
```

This function also locks the mutex before reading the shared value.
That ensures we donâ€™t read it while another goroutine might be writing to it â€” preventing **race conditions** during read operations.

---

### ğŸ‘· Step 4: Inside `main()`

```go
var wg sync.WaitGroup
counter := &Counter{}
numOfGoroutines := 10
```

* We create a **WaitGroup** to wait for all goroutines to finish.
* We create a shared **counter** instance (pointer so that all goroutines access the same one).
* Weâ€™ll run **10 goroutines** to increment it concurrently.

---

### ğŸš€ Step 5: Spawning goroutines

```go
for range numOfGoroutines {
	wg.Add(1)
	go func() {
		defer wg.Done()
		for range 100 {
			counter.increment()
		}
	}()
}
```

Letâ€™s decode this loop:

1. `for range numOfGoroutines`

   * Runs 10 times because `numOfGoroutines` is `10`.

2. Each iteration:

   * Adds 1 to the WaitGroup (`wg.Add(1)`).
   * Launches a **new goroutine** â€” an independent lightweight thread of execution.

3. Inside each goroutine:

   * `defer wg.Done()` ensures that when it finishes, it signals the main goroutine that itâ€™s done.
   * The inner `for range 100` loop runs 100 times, incrementing the counter 100 times **safely**.

So, in total:

```
10 goroutines Ã— 100 increments each = 1000 increments total
```

---

### ğŸ•’ Step 6: Waiting for all goroutines

```go
wg.Wait()
```

* The main goroutine waits here until **all 10 goroutines** have called `wg.Done()`.
* This ensures all increments complete before we move to print the final value.

---

### ğŸ§® Step 7: Printing the final result

```go
fmt.Printf("âœ… Final counter val: %d\n", counter.getVal())
```

At this point:

* All goroutines are finished.
* The mutex guarantees no race conditions.
* The final value of the counter = **1000** (10 Ã— 100).

---

### âœ… Output

```
âœ… Final counter val: 1000
```

---

### ğŸ§  Why mutex is essential here

Without the mutex, multiple goroutines could execute `c.val++` simultaneously.
This looks innocent, but itâ€™s not atomic â€” under the hood, itâ€™s equivalent to:

```go
temp := c.val   // read
temp = temp + 1 // increment
c.val = temp    // write
```

If two goroutines do this at once, oneâ€™s write could **overwrite** anotherâ€™s result, leading to a smaller final count (e.g., 967, 984, etc.).

The mutex ensures these three steps happen **as one atomic block** â€” no interruptions.

---

### ğŸ§© Summary Table

| Step                | Operation | Protected by Mutex?          | Notes |
| ------------------- | --------- | ---------------------------- | ----- |
| Increment (`val++`) | âœ…         | Prevents race condition      |       |
| Read (`getVal()`)   | âœ…         | Ensures consistent read      |       |
| Concurrent access   | âœ…         | Only one goroutine at a time |       |
| Final output        | `1000`    | Always correct               |       |

---

### ğŸ§­ Real-world analogy

Think of the mutex as a **key to a single restroom** ğŸš»:

* Only one person (goroutine) can enter at a time.
* Everyone else must wait until the key is returned (Unlock).
* Once done, the next in line can safely go in.

---

Perfect,  ğŸ”¥ letâ€™s now see **exactly what goes wrong** when we remove the mutex and why the final value becomes unreliable.
Weâ€™ll use our same code, but comment out the mutex parts so we can clearly see the difference in behavior.

---

## âš ï¸ 1ï¸âƒ£ Version WITHOUT Mutex

```go
package main

import (
	"fmt"
	"sync"
)

type Counter struct {
	val int
}

func (c *Counter) increment() {
	// âŒ No mutex protection
	c.val++
}

func (c *Counter) getVal() int {
	return c.val
}

func main() {
	var wg sync.WaitGroup
	counter := &Counter{}
	numOfGoroutines := 10

	for range numOfGoroutines {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for range 100 {
				counter.increment()
			}
		}()
	}

	wg.Wait()
	fmt.Printf("âš ï¸ Final counter val (no mutex): %d\n", counter.getVal())
}
```

---

## ğŸ§  Expected output (if perfectly serialized)

If everything ran in perfect sequence, weâ€™d expect:

```
10 goroutines Ã— 100 increments = 1000
```

So, the final value **should** be `1000`.

---

## ğŸ˜ˆ Actual output (race condition at play)

Run this several times, and youâ€™ll likely see outputs like:

```
âš ï¸ Final counter val (no mutex): 973
âš ï¸ Final counter val (no mutex): 991
âš ï¸ Final counter val (no mutex): 985
âš ï¸ Final counter val (no mutex): 1000
âš ï¸ Final counter val (no mutex): 968
```

The number changes **randomly** on each run.
That randomness is the result of **race conditions**.

---

## ğŸ” Why race conditions occur

Letâ€™s zoom into the line:

```go
c.val++
```

This looks atomic but itâ€™s **actually three CPU-level operations**:

1. **Read** the current value of `c.val` from memory into a register.
2. **Add 1** to it.
3. **Write** the new value back to memory.

Now imagine two goroutines (G1, G2) running in parallel on different CPU cores:

| Step | G1                | G2                | Result             |
| ---- | ----------------- | ----------------- | ------------------ |
| 1ï¸âƒ£  | Reads `c.val = 5` | â€”                 | â€”                  |
| 2ï¸âƒ£  | â€”                 | Reads `c.val = 5` | â€”                  |
| 3ï¸âƒ£  | Increments â†’ `6`  | Increments â†’ `6`  | âŒ One update lost! |

Each thinks it incremented `5 â†’ 6`, but both wrote `6`.
So, one increment **overwrites** the other â€” a classic race condition.

---

## ğŸ•µï¸â€â™‚ï¸ Detecting it with Goâ€™s race detector

Go actually comes with a built-in **race detector**.
Try running your code with:

```
go run -race .
```

Output example:

```
==================
WARNING: DATA RACE
Read at 0x00c0000a6010 by goroutine 8:
  main.(*Counter).increment()
      /path/to/main.go:13 +0x3c
...
Found 1 data race(s)
exit status 66
```

The race detector identifies unsynchronized reads/writes to shared memory and warns us.

---

## ğŸ§© 2ï¸âƒ£ Version WITH Mutex (Safe One)

Now compare it with your earlier code:

```go
func (c *Counter) increment() {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.val++
}
```

Now:

* Only **one goroutine** can perform `c.val++` at a time.
* The others **wait** at `Lock()` until itâ€™s released.
* So, all 1000 increments are guaranteed to complete without data loss.

âœ… Output will **always** be:

```
âœ… Final counter val: 1000
```

---

## ğŸ§  Summary

| Concept                    | Without Mutex                       | With Mutex                      |
| -------------------------- | ----------------------------------- | ------------------------------- |
| Access type                | Concurrent                          | Synchronized                    |
| Data safety                | âŒ Race condition                    | âœ… Safe                          |
| Output consistency         | Unpredictable                       | Always 1000                     |
| CPU instructions (`val++`) | Read â†’ Add â†’ Write (can interleave) | Executes atomically inside Lock |
| Debugging help             | Use `go run -race .`                | Race detector shows no issue    |

---

## ğŸ§© Real-World Analogy

Imagine 10 people (goroutines) trying to **update a whiteboard** at the same time.

* Without rules (mutex): they bump into each other, overwrite numbers, and cause chaos.
* With a rule (mutex): only one person writes at a time while others wait â€” result stays correct.

---

2ï¸âƒ£ **ANOTHER EXAMPLE** ğŸ

This example beautifully shows how **mutexes** protect shared variables even when that variable itself doesnâ€™t belong to a struct. Letâ€™s break it down piece by piece and answer our key questions:

> â€œğŸ’¡ How do mutexes understand which values to protect?â€

---

```go 
package main

import (
	"fmt"
	"sync"
)

//ğŸ’¡mutexes - how do they understand which values to protect?

func main() {

	var counter int
	var wg sync.WaitGroup

	var mu sync.Mutex

	numOfGoroutines:=5
	wg.Add(numOfGoroutines)

	increment:= func ()  {
		// can be used inside loops too
		defer wg.Done()
		for range 1000{
			mu.Lock()
			counter++
			mu.Unlock()
		}
	}

	for range numOfGoroutines{
		go increment()
	}

	wg.Wait()
	fmt.Printf("âœ… Final counter val: %d\n",counter)
	
	// O.P- 
	// $ go run .
	// âœ… Final counter val: 5000
	
}
```

## ğŸ§© Code breakdown

```go
package main

import (
	"fmt"
	"sync"
)
```

We import:

* `fmt` â€” for printing
* `sync` â€” for synchronization tools (`WaitGroup`, `Mutex`)

---

### 1ï¸âƒ£ Shared data and synchronization primitives

```go
var counter int
var wg sync.WaitGroup
var mu sync.Mutex
```

Here we declare three key variables:

| Variable  | Type             | Purpose                                           |
| --------- | ---------------- | ------------------------------------------------- |
| `counter` | `int`            | Shared value accessed by multiple goroutines      |
| `wg`      | `sync.WaitGroup` | Waits for all goroutines to finish                |
| `mu`      | `sync.Mutex`     | Ensures mutual exclusion when accessing `counter` |

---

### 2ï¸âƒ£ Setting number of goroutines

```go
numOfGoroutines := 5
wg.Add(numOfGoroutines)
```

Weâ€™ll run **5 goroutines**, each incrementing the counter 1000 times.

So expected final result =
`5 Ã— 1000 = 5000`.

`wg.Add(numOfGoroutines)` ensures the main goroutine knows to wait for **5 completions**.

---

### 3ï¸âƒ£ Defining the increment function

```go
increment := func() {
	defer wg.Done()
	for range 1000 {
		mu.Lock()
		counter++
		mu.Unlock()
	}
}
```

Letâ€™s dissect this line by line.

#### ğŸ”¸ `defer wg.Done()`

* Marks that when this goroutine finishes, it will signal the WaitGroup.
* So `wg.Wait()` in `main` knows one goroutine is done.

#### ğŸ”¸ `for range 1000`

* Loops 1000 times, so each goroutine performs 1000 increments.

#### ğŸ”¸ `mu.Lock()` and `mu.Unlock()`

* These are **critical section boundaries**.
* Only one goroutine can execute the code between them at a time.

Hereâ€™s what happens under the hood:

1. When a goroutine calls `mu.Lock()`, it **tries to acquire ownership** of that mutex.
2. If the mutex is already locked by another goroutine, this one will **pause (block)**.
3. When the owner calls `mu.Unlock()`, the mutex becomes available.
4. The waiting goroutine then wakes up, acquires the lock, increments `counter`, and unlocks again.

So effectively, only **one goroutine at a time** modifies `counter++`.

---

### 4ï¸âƒ£ Launching goroutines

```go
for range numOfGoroutines {
	go increment()
}
```

This launches 5 goroutines in parallel, all running the `increment` function.

So:

* Each goroutine does `1000` increments.
* Each increment operation is **protected by the same mutex**.

---

### 5ï¸âƒ£ Waiting for all goroutines

```go
wg.Wait()
```

This makes the `main()` function **wait** until all 5 goroutines finish their 1000 increments.

---

### 6ï¸âƒ£ Printing result

```go
fmt.Printf("âœ… Final counter val: %d\n", counter)
```

Once all increments complete, we print the final counter.

Since each of the 5 goroutines adds 1000, and all updates are protected by the mutex, the result is always:

```
âœ… Final counter val: 5000
```

---

## ğŸ” Soâ€¦ how does the mutex â€œknowâ€ which value to protect?

Excellent question â€” and this is subtle.

> The **mutex doesnâ€™t automatically â€œknowâ€** which variable itâ€™s protecting.
> **We** decide what it protects â€” by **where** we place the `Lock()` and `Unlock()` calls.

---

### ğŸ§  Conceptually:

A mutex is like a **key** to a restricted area.
It doesnâ€™t care whatâ€™s inside â€” it just ensures that only one goroutine can enter that section of code at a time.

So in this example:

```go
mu.Lock()
counter++
mu.Unlock()
```

* The **critical section** is everything between `Lock()` and `Unlock()`.
* Since `counter++` happens inside this section, the mutex is effectively **protecting the counter**.

If you had this instead:

```go
mu.Lock()
x++
y++
mu.Unlock()
```

Then the same mutex would protect **both `x` and `y`**.

In short:

> The protection scope is determined by the **code block between Lock and Unlock**, not by the variable type or name.

---

### ğŸ§© Timeline visualization

Letâ€™s see how goroutines take turns on the mutex:

| Time | Goroutine | Action                    |
| ---- | --------- | ------------------------- |
| t1   | G1        | Lock â†’ Increment â†’ Unlock |
| t2   | G2        | Lock â†’ Increment â†’ Unlock |
| t3   | G3        | Lock â†’ Increment â†’ Unlock |
| t4   | G4        | Lock â†’ Increment â†’ Unlock |
| t5   | G5        | Lock â†’ Increment â†’ Unlock |

Even though theyâ€™re all running concurrently, the mutex **serializes** access to the shared variable.

---

### ğŸ§® Verification

Expected:

```
5 goroutines Ã— 1000 increments = 5000
```

Output:

```
âœ… Final counter val: 5000
```

Always consistent â€” because the mutex prevents any data races.

---

### ğŸ’¬ Quick recap

| Concept          | Explanation                                                              |
| ---------------- | ------------------------------------------------------------------------ |
| Mutex            | A lock that ensures only one goroutine can access a code block at a time |
| How it â€œknowsâ€   | It doesnâ€™t â€” it just locks *that piece of code* where we place it        |
| Critical section | Code between `Lock()` and `Unlock()`                                     |
| Why it works     | Prevents interleaving of operations like `counter++`                     |
| Output           | Always 5000 â€” no race conditions                                         |

---






